Descriptions of bicycle simulations

SHAPING Definitions
0 : no shaping, bicycle gets +1 reward if has not fallen at a state
1: shaping with lean and lean rate. (1-(abs(phi)) - np.sign(phi)*phi_dot/10)
Shaping was later subsummed into reward_flag


***** SIMULATIONS *****

VI2: Value Iteration 2. 11/7/18. Shaping of 1 - abs(phi) - phi_dot*sign(phi)/10.
    Linear Interpolation to get values for states. 30 episodes of value iteration.
    Grid Discritization of 0.
    Bicycle Balanced for all 10s!!
    Sharpe Jumps in steer angle command. This is to be expected.

VI3: Value Iteration 3. 11/7/18. No shaping (shaping 0)
    Linear Interpolation to get values for states. 30 epsiodes of value iteration.
    Grid Discritization of 0
    Bike balances for ~1.5 seconds.

VI4: Value Iteration 3. 11/7/18. Shaping = 1. No interpolation (to get the value
    of a state, take the value of the nearest state). 30 training episodes.
    Grid Discritization 0.
    Bike balances for 0.68s

VI5: Value Iteration 5. 11/7/18. Shaping = 0 (No shaping). Linear Interpolation.
    100 episodes, Grid Discritization 0.
    Bike Balances for ~1.5s

VI6: Value ITeration 6. 11/7/18. Reward_flag = 1. Linear interpolation.
    state_grid_flag = 1. action_grid_flag = 2

VI7: Value ITeration 7. 11/7/18. Reward_flag = 2. State_grid_flag = 1,
    action_grid_flag = 0.

VI8: Value Iteration. action_flag = 2 (100 actions). State_grid_flag = 0
    Reward_flag = 1?

VI9: Value Iteration. continuous actions only. state_grid_flag = 1,
    reward_flag = 3. trained for 30 episodes. Bad performance.
        - not much differentiation in utility values, not much differentiation
            in actions.

VI10: Value Iteration. Continuous actions only. state_grid_flag = 0,
        BUG IN REWARD
    reward_flag = 3. trained for 10 episodes.
    Bad results. Bike balanced for 0.3 seconds.

Heatmap policy interpolation continuous,
    Continuous actions for policy using iterpolation. based on VI8 which used
        discrete actions to calculate utility values.
        Rough comparision to linear controller.

VI11: similiar to VI10, but with dicsrete actions during training.
        BUG IN REWARD
    state_grid_flag = 0
    reward_flag = 3. action_Grid_flag = 3. trained for 10 episodes.
    continuous actions during testing.
    Bad utility heatmap. Bad policy heatmap.

VI12: similiar to VI11. but fixed problem with reward_for_Falling (set it back)
    to 0 so that rewards are propogatted properly.

VI13. similiar to VI12. state_grid_flag = 0. action_grid_flag = 3.
    reward_flag = 3. trained using discrete actions.
    Trained for 30 episodes.
    Tested with continuous and discrete actions.

VI14. similiar to VI13. state_grid_flag = 0. action_grid_flag = 3.
    reward_flag = 3. trained using continuous actions.
    Trained for 30 episodes.
    Tested with continuous and discrete actions.
    11/24/18

VI15. similiar to VI14. state_grid_flag = 0. action_grid_flag = 3.
    reward_flag = 4 (abs phi). trained using continuous actions.
    Trained for 30 episodes.
    Tested with continuous and discrete actions.
    11/24/18

V16: Value Iteration. action_flag = 2 (100 actions). State_grid_flag = 0
    Reward_flag = 1. trained with discrete actions.
    30 training episodes. similiar to vi8

VI17: Value Iteration. action_flag = 2 (100 actions). State_grid_flag = 0.
    Reward_flag = 1. trained with discrete actions. Should be similiar to VI14 or VI8/ Gamma Training = 0.95

VI18: VAlue Iteration. action_flag = 2 (100 actions). State_grid_flag = 0.
    Reward_flag = 1. trained with discrete actions. Should be similiar to VI14 or VI8/ Gamma Training = 1

VI19: VAlue Iteration. action_flag = 2 (100 actions). State_grid_flag = 0.
    Reward_flag = 3. trained with discrete actions. Should be similiar to VI18 but with reward flag actually equal to 3. Gamma Training = 1

VI20: Value Iteration. action_flag = 2 (100 actions). State_grid_flag = 0.
    Reward_flag = 3. trained with discrete actions. Should be similiar to VI17 with reward flag actually equal to 3. Gamma Training = 0.95

--------------------- CIRCLE DEBUGGING: -----------------------
Test1:
    VI8, reward_flag =3, action_grid_flag = 0, state_grid_flag = 0
    2 with state_flag1 = 3. gamma_training = 0.95, 2 with state_flag2. one each with continuous and discrete actions. trained with discrete actions.
    Ie, test starting with both a positive and negative intial lean angle (of the same value)

VI21:
    state_grid_flag = 1. action_grid_flag = 1 (11 actions). reward_flag = 3.
    gamma_training = 0. trained with discrete actions.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 30 episodes

VI22:
    state_grid_flag = 1. action_grid_flag = 1 (11 actions). reward_flag = 3.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 30 episodes. trained with discrete actions.
    same as VI21, but reversed the order for each index (but kept the order
    we looped through state variables the same)

VI23:
    state_grid_flag = 1. gamma_training = 0.95, action_grid_flag = 1 (11 actions). reward_flag = 3. trainged with discrete actions.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 30 episodes
    same as VI21, but reversed the order for each index and reversed the order I loop through states

VI24:
    state_grid_flag = 1. gamma_training = 0.95, action_grid_flag = 1 (11 actions). reward_flag = 3. trained with discrete actions.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 100 episodes
    same as VI23 but trained for 100 iterations.


VI25:
    state_grid_flag = 1. action_grid_flag = 1 (11 actions). reward_flag = 3.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 250 episodes. trained with discrete actions.
    gamma_train = 0.95 (is this right)
    same as VI24 but trained for 250 iterations.

VI26:
    state_grid_flag = 1. gamma_training = 0.95, action_grid_flag = 1 (11 actions). reward_flag = 3. trained with continuous actions.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 100 episodes
    same as VI24 but trained with continuous actions.

VI27:
    state_grid_flag = 1. gamma_training = 0.95, action_grid_flag = 1 (11 actions). reward_flag = 3. trainged with discrete actions.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 30 episodes
    same as VI23, but loops through state indicies in random order.

VI28:
    state_grid_flag = 1. gamma_training = 0.95, action_grid_flag = 1 (11 actions). reward_flag = 3. trainged with discrete actions.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 100 episodes
    same as VI27, but with 100 episodes.

~VI29:
    state_grid_flag = 1. gamma_training = 0.95, action_grid_flag = 1 (11 actions). reward_flag = 3. trained with discrete actions.
    tested with state_flag = 3 and state_flag = 5 (lean angle has reverse sign). trained 100 episodes
    same as VI28, but with 500 episodes.
---------------------------   Timing  -------------------------------------

-VI31: timing tests.
    Actions grid flag = 0: 5 actions.
    State grid flag = 2: 9x7x7 actions.
    does interpolation for training, but does not use continuous actions
    10 episodes: 16.14 sec, 22.8 sec, 20.6s, 19.75s

VI32: timint tests
    action grid flag = 0: 5 actions.
    state grid flag = 3: 11x9x7 actions
    discrete actions, interpolation

VI33: timint tests
    action grid flag = 1: 11 actions.
    state grid flag = 1:
    discrete actions, interpolation

-------------------------  Smooth Controller Development   ------------------

VI34: smooth action tests
    action grid flag = 1: 11 actions.
    state grid flag = 4: 19x17x15 states
    reward flag = 3
    discrete actions, interpolation
    trained for 30 episodes

VI35: VI34 but with 100 actions
    action grid flag = 2: 101 actions.
    state grid flag = 4: 19x17x15 states
    reward flag = 3
    discrete actions, interpolation
    trained for 30 episodes

VI36: VI34 but with 100 episodes
    action grid flag = 2: 101 actions.
    state grid flag = 4: 19x17x15 states
    reward flag = 3
    discrete actions, interpolation
    trained for 100 episodes

VI37: VI34 but with 500 episodes
    action grid flag = 2: 101 actions.
    state grid flag = 4: 19x17x15 states
    reward flag = 3
    discrete actions, interpolation
    trained for 500 episodes

VI38: VI34 but with reward_flag = 5
    action grid flag = 1: 1 actions.
    state grid flag = 4: 19x17x15 states
    reward flag = 5 (penalize all three states)
    discrete actions, interpolation
    trained for 30 episodes

VI39: VI38 but trained for 100 episodes
    action grid flag = 1: 11 actions.
    state grid flag = 4: 19x17x15 states
    reward flag = 5 (penalize all three states)
    discrete actions, interpolation
    trained for 100 episodes

VI40: VI34, but with tighter state space
    action grid flag = 1: 11 actions.
    state grid flag = 6 : 27x29x25 states
    reward flag = 3
    discrete actions, interpolation
    trained for 30 episodes

--------------------   changed reward to include action ----------
started saving things in modelsB directory

VI41: same as VI34, but testing reward = r(s,a)
    action grid flag = 1: 11 actions.
    state grid flag = 4: 19x17x15 states
    reward flag = 3
    discrete actions, interpolation
    trained for 30 episodes

VI42: small state space
    action grid flag = 0: 5 actions.
    state grid flag = : 11x9x7 states
    reward flag = 3
    discrete actions, interpolation
    trained for 30 episodes

VI43: working
    action grid flag = 1: 11 actions.
    state grid flag = 4 :
    reward flag = 3
    discrete actions, interpolation
    trained for 30 episodes
    gamma = 1

------- smooth controller development continued ---------------

VI44: trained with continuous actions
    action grid flag = 4
    state grid flag = 4 :
    reward flag = 3
    continuous actions, interpolation
    trained for 30 episodes
    gamma = 1

VI45: penalize actions
    action grid flag = 1: 11 actions.
    state grid flag = 4 : 19 x 17 x 15 states
    reward flag = 6
    discrete actions, interpolation
    trained for 30 episodes
    gamma = 1

** THIS IS THE GOOD TEST: VI46-s8 **
VI46: less regularization
    action grid flag = 1: 11 actions.
    state grid flag = 4 : 19 x 17 x 15 states
    reward flag = 7
    discrete actions, interpolation
    trained for 30 episodes
    gamma = 1


VI47: lesser regularization
    action grid flag = 1: 11 actions.
    state grid flag = 4 : 19 x 17 x 15 states
    reward flag = 8
    discrete actions, interpolation
    trained for 30 episodes
    gamma = 1

VI48: lesser regularization
    action grid flag = 1: 11 actions.
    state grid flag = 4 : 19 x 17 x 15 states
    reward flag = 9
    discrete actions, interpolation
    trained for 30 episodes
    gamma = 1

VI49:
    action grid flag = 1: 11 actions.
    state grid flag = 4 : 19 x 17 x 15 states
    reward flag = 1
    discrete actions, interpolation
    trained for 30 episodes
    gamma = 1

VI50-s8_lin: similiar to VI46 but with r=11, pd Q matrix. Linear EOM!
    action grid flag = 1: 11 actions.
    state grid flag = 8 : 19 x 17 x 15 states
    reward flag = 11
    discrete actions, interpolation
    trained for 30 episodes
    gamma = 1


VI43, VI45, VI46 -100 variants trained for 100 episodes
 - used state grid flag = 3 for faster testing
    (VI46 version accidentally named VI46-11)

VI43, VI45, VI46 -s6 variants use state_flag = 6. Tight, small state space
 - trained for 30 episodes

 VI43, VI45, VI46 -s6a2 variants use state_flag = 6 and action_flag = 2.
  Tight, small state space, many actions
 - trained for 30 episodes

-s8 variants: used with state flag 8
-s9 : used state flag 9 :
-s11: use state flag 11
-linear variants: trained using linear EoMs. (can be tested on nonlinear EoMs)


-s10: same size grid as s8, but with uneven spacing, more logrithmic
-a5: with more actions, more logrithmic ish spacings

-a2: 101 actions

 -s6cont variants use state_flag =6 and are trained with continuous actions

VI_r14_a1_s16
    discrete actions, interpolation
    trained for 20 episodes
    gamma = 1






# I SHOULD WRITE THING TO AUTOMATICALLY GENERATE THESE. Automate
Q2: Q Learning 2. 11/



***********************
